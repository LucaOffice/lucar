% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prepare_logdata.R
\name{prepare_logdata}
\alias{prepare_logdata}
\title{Reads all JSON files into a single dataframe}
\usage{
prepare_logdata(
  path = "./",
  summarize_wf = FALSE,
  unzip = FALSE,
  workflow_codes = lucar::workflow_coding,
  tool_codes = lucar::tool_coding,
  debug_mode = FALSE
)
}
\arguments{
\item{path}{The path to the folder including all JSON files (files in subfolders are also considered).}

\item{summarize_wf}{If TRUE the events with identical task codes and directly following each other will be summarized to a single task}

\item{unzip}{If true, the function looks for zip archives located in the given path, corresponding to the naming convention for exported data from LUCA Office, and unzips these.}

\item{workflow_codes}{Dataframe with the workflow coding that is used to structure the log data}

\item{tool_codes}{Dataframe with the tool coding that is used to assign each used tool to a common code}

\item{debug_mode}{If TRUE the results include the internal hash IDs for the project elements are included and a tibble including unknown event types (if there were any). If 'scenario_specific' is set to TRUE it will be enforced to 'FALSE'.}
}
\value{
A dataframe including the prepared data from all JSON files
}
\description{
JSON files exported from the LUCA office simulation into a dedicated folder
are read into a datafrmame.
Best is to dowload all zip files into a dedicated folder and unpack them in that same folder.
Then provide this folder's path to the function.
}
\examples{

# Searches in the current working directory and all subdirectories for log data from LUCA office
# and prepares the data in a structure suitable for further analyses
\dontrun{
logdata <- prepare_logdata()
}

}
